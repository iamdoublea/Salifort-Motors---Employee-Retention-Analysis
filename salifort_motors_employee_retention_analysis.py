# -*- coding: utf-8 -*-
"""Salifort Motors - Employee Retention Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbPG2wcg6QG7F_sxsPtksGunHBM6oQwT

# **Salifort Motors - Employee Retention Analysis**

### **Providing data-driven suggestions for HR**

## Description and deliverables

This project will help us analyze a dataset and build predictive models that can provide insights to the Human Resources (HR) department of a large consulting firm.

Upon completion, we will provide a brief one-page summary of this project that you would present to external stakeholders as the data professional in Salifort Motors. The other is a complete code notebook provided here.

### Understand the business scenario and problem

The HR department at Salifort Motors wants to take some initiatives to improve employee satisfaction levels at the company. They have the following question: what’s likely to make the employee leave the company?

Your goals in this project are to analyze the data collected by the HR department and to build a model that predicts whether or not an employee will leave the company.

If you can predict employees likely to quit, it might be possible to identify factors that contribute to their leaving. Because it is time-consuming and expensive to find, interview, and hire new employees, increasing employee retention will be beneficial to the company.

### Familiarize yourself with the HR dataset

The dataset that you'll be using in this lab contains 15,000 rows and 10 columns for the variables listed below.

**Note:** For more information about the data, refer to its source on [Kaggle](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv).

Variable  |Description |
-----|-----|
satisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|
last_evaluation|Score of employee's last performance review [0&ndash;1]|
number_project|Number of projects employee contributes to|
average_monthly_hours|Average number of hours employee worked per month|
time_spend_company|How long the employee has been with the company (years)
Work_accident|Whether or not the employee experienced an accident while at work
left|Whether or not the employee left the company
promotion_last_5years|Whether or not the employee was promoted in the last 5 years
Department|The employee's department
salary|The employee's salary (U.S. dollars)

[Double-click to enter your responses here.]

### Import packages
"""

# Import packages

import numpy as np
import pandas as pd

# few for visulaisation

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# RUN THIS CELL TO IMPORT YOUR DATA.

# Load dataset into a dataframe

df0 = pd.read_csv("HR_capstone_dataset.csv")


# Display first few rows of the dataframe

df0.head()

"""### Data Exploration (Initial EDA and data cleaning)

- Understand your variables
- Clean your dataset (missing data, redundant data, outliers)


"""

# Gather basic information about the data

df0.shape

# the data contains 14999 rows and 10 columns

## Checking the datatypes and other infos

df0.info()

# We have got a total of 2 categorical variables and 8 numeric variables

"""### Gather descriptive statistics about the data"""

# Gather descriptive statistics about the data
df0.describe()

## this is a very high level statical summary of the dataset numeric variables

# at the first glance we can see that most of the columns are having almost normal distrubution

"""### Rename columns

As a data cleaning step, rename the columns as needed. Standardize the column names so that they are all in `snake_case`, correct any column names that are misspelled, and make column names more concise as needed.
"""

# Display all column names
### YOUR CODE HERE ###
df0.columns

# Rename columns as needed

df0.rename(columns={'Work_accident': 'work_accident',
                          'average_montly_hours': 'average_monthly_hours',
                          'time_spend_company': 'tenure',
                          'Department': 'department'},inplace=True)

# Display all column names after the update

df0.columns

"""### Check missing values

Check for any missing values in the data.
"""

# Check for missing values

df0.isnull().sum()

# there is no missing values in the dataset

"""### Check duplicates

Check for any duplicate entries in the data.
"""

# Check for duplicates

df0.duplicated().sum()

# there are 3008 duplicated values int the dataset.

# Inspect some rows containing duplicates as needed

df0[df0.duplicated()]

"""The above output shows the first five occurences of rows that are duplicated farther down in the dataframe. How likely is it that these are legitimate entries? In other words, how plausible is it that two employees self-reported the exact same response for every column?
It seems very unlikely that these observations are legitimate. We can proceed by dropping them.
"""

# Drop duplicates and save resulting dataframe in a new variable as needed

df0 = df0.drop_duplicates(keep='first')

#making a copy
df1 = df0.copy()

# Display first few rows of new dataframe as needed

df1.head()

# Checking for duplicates to confirm

df1.duplicated().sum()

"""### Check outliers

Check for outliers in the data.
"""

# Create a boxplot to visualize distribution of `tenure` and detect any outliers

sns.boxplot(df1['tenure'])
plt.show()

# We can definitely see few outliers,beyind the upper limit

# Determine the number of rows containing outliers

Q1 = df1['tenure'].quantile(0.25)
Q3=df1['tenure'].quantile(0.75)

IQR = Q3-Q1

upper_bound = Q3 + 1.5*IQR
lower_bound = Q1 - 1.5*IQR

outliers = df1[(df1['tenure']>upper_bound) | (df1['tenure']<lower_bound)]

print(len(outliers))

# 824 rows are having outliers

"""Certain types of models are more sensitive to outliers than others. When you get to the stage of building your model, consider whether to remove outliers, based on the type of model you decide to use.

## Step 2. Data Exploration (Continue EDA)

Begin by understanding how many employees left and what percentage of all employees this figure represents.
"""

# Get numbers of people who left vs. stayed

print(df1['left'].value_counts())

# Get percentages of people who left vs. stayed

print(df1['left'].value_counts(normalize=True))

# We can see that around 17% of the employees left the organisation and the rest stayed, also we realised one more thing, that the target variable is not balanced, that means one section has more value than the other, so we migh to fix that later if needed.

"""### Data visualizations

Now, examine variables that you're interested in, and create plots to visualize relationships between variables in the data.
"""

# Create a plot as needed
# So we are going to check aech and individual columns and see if all are okay for traing the madel or not

## Statisfaction level

# check the distribution

sns.histplot(df1['satisfaction_level'],kde=True)

# initial thought, it is not a proper normally distributted curve, it is not even proper left skewed

sns.boxplot(df1['satisfaction_level'])
plt.show()

# no outliers

# Create a plot as needed

## last_evaluation

# distribution
sns.histplot(df1['last_evaluation'],kde=True)
plt.show()

# Mostly a equally distributed distribution

# Ouliers check

sns.boxplot(df1['last_evaluation'])
plt.show()

# No such outliers

# Create a plot as needed

## project_number

# Distribution

sns.histplot(df1['number_project'],kde=True)
plt.show()

df1['number_project'].value_counts()

# Outleirs


sns.boxplot(df1['number_project'])
plt.show()

# no such outliers

# Create a plot as needed
## average_monthly_hours
sns.histplot(df1['average_monthly_hours'],kde=True)
plt.show()

# Oulier check

sns.boxplot(df1['average_monthly_hours'])
plt.show()

# no such outliers

# Create a plot as needed
## Work accident

# Distribution

sns.histplot(df1['work_accident'])
plt.show()

df1['work_accident'].value_counts()

# Create a plot as needed
## promotions_last_5years

# distribution

sns.histplot(df1['promotion_last_5years'])
plt.show()

df1['promotion_last_5years'].value_counts()

# Create a plot as needed
## department

# distribution

sns.histplot(df1['department'])
plt.xticks(rotation=45)
plt.show()

df1['department'].value_counts()

# Create a plot as needed
## Salary

# distribution

df1['salary'].value_counts()

df1.columns

# trying to get if there is nay correlation between working hours and porject done and the final output

df1.corr()

plt.figure(figsize=(15,8))
sns.heatmap(df1.corr(),vmin=-1, vmax=1, annot=True, cmap=sns.color_palette("vlag", as_cmap=True))
plt.show()

# As we thought tenure and average monthly hours are somewhat correlated with the decision of the employee to leave or not

sns.boxplot(data=df1,x='average_monthly_hours',y='number_project',hue='left',orient="h")
plt.show()

"""This is showing the average monthly hours worked and the number of project an employee is envolved in. Now it is obvious that the more the project he is involved in the more he/she is working that mean the average number of working hours will be high and we can see that too.


But one more we noticed that the people who have worked for more the 5 project are more prone to leave and any one who is working on 7 projects has left.


There is one more group of people who has left are people who has worke in very few number of projects and also for fewer hours but still left. One assumption might be that these people has been fired due to the same or may be they were notice period for which worrked less.

The optimum number of project of the employee is looking to be somewhere around 3-4


One thing is certain that people who are wokring in more than 7 projects will leave, mostly due to the fact that they are overworked.
"""

# to confirm the above insights

df1[df1['number_project']==7]['left'].value_counts()

# As we can see that 145 people who has worked in 7 different projects has left the orhanisation

## New graphs

# Checking the link between average working hours and satisfaction leevl of employees

plt.figure(figsize=(15,8))
sns.scatterplot(data=df1, x='average_monthly_hours', y='satisfaction_level', hue='left', alpha=0.4)
plt.show()

"""### Insights

There are quite a few things that can be derived from this scatterplot.

One thing is for sure that people who are working for 230-240 hours monthly, has a very high tendency to give very less score in satisfaction level and also they are more prone to leaving the organisation.


Overall the data feels a little odd, may be this is a synthetic data
"""

## New graph

# Satisfaction level and Tenure, to see how satisfaction level is correlated with tenure

plt.figure(figsize=(20,8))
sns.boxplot(data=df1, x='satisfaction_level', y='tenure', hue='left', orient="h")
plt.show()

"""Now there are few things that we can understand.

Clearly, poeple how has stayed beyond 6 years and very likely to stay, also there satisfaction level is higher.

And there is pretty decent number of poeple who leaves after 2 years, and this can be understood, since they might get new opportunity and decised to move, since there overall satisfaction level is also decent.

But, surprising, on 4th year, people left and there satisfaction level is very low, so there might some problem there, we will have to dig further to know the real reason.

"""

## Lets check the mean and median score of people who left and who stayed

df1.groupby(by='left')['satisfaction_level'].agg(['mean','median']).rename(columns={'mean':'mean_satisfaction','median':'median_satisfaction'})

"""It is very clear and evident that mean and median satisfaction for people who left are low. Around below 0.50 points"""

## We will check the salary level of different tenure employees

# Create a plot as needed
### YOUR CODE HERE ###

# Set figure and axes
fig, ax = plt.subplots(1, 2, figsize = (22,8))

# Define short-tenured employees
tenure_short = df1[df1['tenure'] < 7]

# Define long-tenured employees
tenure_long = df1[df1['tenure'] > 6]

# Plot short-tenured histogram
sns.histplot(data=tenure_short, x='tenure', hue='salary', discrete=1,
             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.5, ax=ax[0])
ax[0].set_title('Salary histogram by tenure: short-tenured people', fontsize='14')

# Plot long-tenured histogram
sns.histplot(data=tenure_long, x='tenure', hue='salary', discrete=1,
             hue_order=['low', 'medium', 'high'], multiple='dodge', shrink=.4, ax=ax[1])
ax[1].set_title('Salary histogram by tenure: long-tenured people', fontsize='14');

"""We can see that it is not veru likely that people who are staying in the company for very long are having very high salary. Ther are mostly comprised of Mid level salary"""

## Lets find out if there is any realtionship between high evaluation score and average monthly hours worked


# Create scatterplot of `average_monthly_hours` versus `last_evaluation`
plt.figure(figsize=(10, 7))
sns.scatterplot(data=df1, x='average_monthly_hours', y='last_evaluation', hue='left')
plt.title('Monthly hours by last evaluation score', fontsize='14');

"""It is almost obvious that people who are giving in maximum amount of working hours will have some of the highest rating in evaluation, but it is also seen that poeple who are working way too much are pretty prone to Churn.

But here in this data, working higher hours doesnot really guarantee best score in evaluation because there are plenty of poeple with more than 250 hours average, have got really bad scores.

Also, we can see that people will lowest average working hours are getting bad evauation and these poeple are also leaving the organisation
"""

## Lets see if working more hours can guarantee promotion in this organisation or not

# Create plot to examine relationship between `average_monthly_hours` and `promotion_last_5years`
plt.figure(figsize=(16, 3))
sns.scatterplot(data=df1, x='average_monthly_hours', y='promotion_last_5years', hue='left')
plt.title('Monthly hours by promotion last 5 years', fontsize='14');

"""We can see a lot of people getting promoted here just by working more hours.
On the other side, working more hours can very much lead to more churn.
"""

## lets check which department has the highest number of churn

# A Histogram

plt.figure(figsize=(16,8))
sns.histplot(df1,x='department',hue='left',hue_order=[0, 1],multiple='dodge')
plt.show()

"""Comparatively, sales and technical team are facing major churns, although have good number of people who are staying back.

So, department doesnot really play a role in the churn probability rate

### Insights

It seems that employee departures from the company are linked to management issues. The decision to leave is associated with extended working hours, involvement in numerous projects, and overall lower levels of satisfaction. The combination of long hours without corresponding recognition in the form of promotions or positive evaluation scores can lead to dissatisfaction. A significant portion of the company's workforce may be experiencing burnout as a result. Additionally, there's an indication that employees who have been with the company for over six years are less likely to resign.
"""



"""### Recall model assumptions

**Logistic Regression model assumptions**
- Outcome variable is categorical
- Observations are independent of each other
- No severe multicollinearity among X variables
- No extreme outliers
- Linear relationship between each X variable and the logit of the outcome variable
- Sufficiently large sample size

### Model Building, Results and Evaluation
- Fit a model that predicts the outcome variable using two or more independent variables
- Check model assumptions
- Evaluate the model

The goal is to predict whether an employee leaves the company, which is a categorical outcome variable. So this task involves classification. More specifically, this involves binary classification, since the outcome variable `left` can be either 1 (indicating employee left) or 0 (indicating employee didn't leave).

### Identify the types of models most appropriate for this task.

Since the variable you want to predict (whether an employee leaves the company) is categorical, we could either build a Logistic Regression model, or a Tree-based Machine Learning model, overall a classification model would work the best.

### Modeling

This approach covers implementation of Logistic Regression.

## Logistic Regression Model
"""

## importing libraries

from sklearn.linear_model import LogisticRegression

"""Note that binomial logistic regression suits the task because it involves binary classification.


Before splitting the data, we will encode the non-numeric variables. There are two: `department` and `salary`.

`department` is a categorical variable, which means you can dummy it for modeling.

`salary` is categorical too, but it's ordinal. There's a hierarchy to the categories, so it's better not to dummy this column, but rather to convert the levels to numbers, 0&ndash;2.
"""

# Copy the dataframe
df_enc = df1.copy()

# Encoding
# Encode the `salary` column as an ordinal numeric category
df_enc['salary'] = (
    df_enc['salary'].astype('category')
    .cat.set_categories(['low', 'medium', 'high'])
    .cat.codes
)

# Dummy encode the `department` column
df_enc = pd.get_dummies(df_enc, drop_first=False)

df_enc.head(5)

# Lets create a heatmap to visualize how correlated variables are.

sns.heatmap(df_enc[['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']]
            .corr(), annot=True, cmap="crest")

## lets treat the outliers

df_logreg = df_enc[(df_enc['tenure'] >= lower_bound) & (df_enc['tenure'] <= upper_bound)]

# Display first few rows of new dataframe
df_logreg.head()

## Sepearting the individual and dependent variables

X = df_logreg.drop(columns='left',axis=1)
y = df_logreg['left']

## splitting the data for train test

from sklearn.model_selection import train_test_split


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=42)

## Logistic regression training

from sklearn.linear_model import LogisticRegression

log_clf = LogisticRegression(random_state=42, max_iter=500)

# fitting mode

log_clf.fit(X_train,y_train)

## getting predicted result from the model

y_pred = log_clf.predict(X_test)

## To visualise the result

from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,f1_score,precision_score,ConfusionMatrixDisplay,classification_report

# Compute values for confusion matrix
log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)

# Create display of confusion matrix
log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm,
                                  display_labels=log_clf.classes_)

# Plot confusion matrix
log_disp.plot(values_format='')

# Display plot
plt.show()


#print
print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test, y_pred))

"""While the model performs well in predicting employees who did not leave the company (class 0), its performance in predicting employees who did leave (class 1) is relatively lower, with lower precision, recall, and F1-score.

It is giving a high number of false negative, which can be fixed further.



The upper-left quadrant displays the number of true negatives.
The upper-right quadrant displays the number of false positives.
The bottom-left quadrant displays the number of false negatives.
The bottom-right quadrant displays the number of true positives.

True negatives: The number of people who did not leave that the model accurately predicted did not leave.

False positives: The number of people who did not leave the model inaccurately predicted as leaving.

False negatives: The number of people who left that the model inaccurately predicted did not leave

True positives: The number of people who left the model accurately predicted as leaving

A perfect model would yield all true negatives and true positives, and no false negatives or false positives.
"""

#Let us check the imbalance in data

df_logreg['left'].value_counts(normalize=True)

# it is little imbalanced but not too severe, so we are not going for resampling.

"""The above classification report above shows that the logistic regression model achieved a precision of 79%, recall of 82%, f1-score of 80% (all weighted averages), and accuracy of 82%. However, if it's most important to predict employees who leave, then the scores are significantly lower.

## Decision Tree Model

###### Decision Tree Round 1
"""

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(random_state=0)

# Assign a dictionary of hyperparameters to search over
cv_params = {'max_depth':[4, 6, 8, None],
             'min_samples_leaf': [2, 5, 1],
             'min_samples_split': [2, 4, 6]
             }

# Assign a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}

from sklearn.model_selection import GridSearchCV

# initialision GCV

tree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tree1.fit(X_train, y_train)

# Check best parameters
tree1.best_params_

# Check best AUC score on CV
tree1.best_score_

## This is a strong AUC score, which shows that this model can predict employees who will leave very well.

def make_results(model_name:str, model_object, metric:str):
    '''
    Arguments:
        model_name (string): what you want the model to be called in the output table
        model_object: a fit GridSearchCV object
        metric (string): precision, recall, f1, accuracy, or auc

    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores
    for the model with the best mean 'metric' score across all validation folds.
    '''

    # Create dictionary that maps input metric to actual metric name in GridSearchCV
    metric_dict = {'auc': 'mean_test_roc_auc',
                   'precision': 'mean_test_precision',
                   'recall': 'mean_test_recall',
                   'f1': 'mean_test_f1',
                   'accuracy': 'mean_test_accuracy'
                  }

    # Get all the results from the CV and put them in a df
    cv_results = pd.DataFrame(model_object.cv_results_)

    # Isolate the row of the df with the max(metric) score
    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]

    # Extract Accuracy, precision, recall, and f1 score from that row
    auc = best_estimator_results.mean_test_roc_auc
    f1 = best_estimator_results.mean_test_f1
    recall = best_estimator_results.mean_test_recall
    precision = best_estimator_results.mean_test_precision
    accuracy = best_estimator_results.mean_test_accuracy

    # Create table of results
    table = pd.DataFrame()
    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'F1': [f1],
                          'accuracy': [accuracy],
                          'auc': [auc]
                        })

    return table

# Get all CV scores
tree1_cv_results = make_results('decision tree cv', tree1, 'auc')
tree1_cv_results

"""All of these scores from the decision tree model are strong indicators of good model performance.

Recall that decision trees can be vulnerable to overfitting, and random forests avoid overfitting by incorporating multiple trees to make predictions. You could construct a random forest model next.

###### Random Forest - Round 1
"""

## random forest with gridsearch cv

from sklearn.ensemble import RandomForestClassifier

# Instantiate model
rf = RandomForestClassifier(random_state=0)

# Assign a dictionary of hyperparameters to search over
cv_params = {'max_depth': [3,5, None],
             'max_features': [1.0],
             'max_samples': [0.7, 1.0],
             'min_samples_leaf': [1,2,3],
             'min_samples_split': [2,3,4],
             'n_estimators': [300, 500],
             }

# Assign a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}

# Instantiate GridSearch
rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf1.fit(X_train, y_train) # --> Wall time: ~10min

# Define a path to the folder where you want to save the model
path = '/home/jovyan/work/'

import pickle

def write_pickle(path, model_object, save_as:str):
    '''
    In:
        path:         path of folder where you want to save the pickle
        model_object: a model you want to pickle
        save_as:      filename for how you want to save the model

    Out: A call to pickle the model in the folder indicated
    '''

    with open(path + save_as + '.pickle', 'wb') as to_write:
        pickle.dump(model_object, to_write)

def read_pickle(path, saved_model_name:str):
    '''
    In:
        path:             path to folder where you want to read from
        saved_model_name: filename of pickled model you want to read in

    Out:
        model: the pickled model
    '''
    with open(path + saved_model_name + '.pickle', 'rb') as to_read:
        model = pickle.load(to_read)

    return model

# Write pickle
write_pickle(path, rf1, 'hr_rf1')

# Read pickle
rf1 = read_pickle(path, 'hr_rf1')

# Check best AUC score on CV
rf1.best_score_

# Check best params
rf1.best_params_

# Get all CV scores
rf1_cv_results = make_results('random forest cv', rf1, 'auc')
print(tree1_cv_results)
print(rf1_cv_results)

"""The evaluation scores of the random forest model are better than those of the decision tree model, with the exception of recall (the recall score of the random forest model is approximately 0.001 lower, which is a negligible amount). This indicates that the random forest model mostly outperforms the decision tree model.

Next, you can evaluate the final model on the test set.
"""

from sklearn.metrics import roc_auc_score

def get_scores(model_name:str, model, X_test_data, y_test_data):
    '''
    Generate a table of test scores.

    In:
        model_name (string):  How you want your model to be named in the output table
        model:                A fit GridSearchCV object
        X_test_data:          numpy array of X_test data
        y_test_data:          numpy array of y_test data

    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model
    '''

    preds = model.best_estimator_.predict(X_test_data)

    auc = roc_auc_score(y_test_data, preds)
    accuracy = accuracy_score(y_test_data, preds)
    precision = precision_score(y_test_data, preds)
    recall = recall_score(y_test_data, preds)
    f1 = f1_score(y_test_data, preds)

    table = pd.DataFrame({'model': [model_name],
                          'precision': [precision],
                          'recall': [recall],
                          'f1': [f1],
                          'accuracy': [accuracy],
                          'AUC': [auc]
                         })

    return table

# Get predictions on test data
rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)
rf1_test_scores

"""The test scores are very similar to the validation scores, which is good. This appears to be a strong model. Since this test set was only used for this model, you can be more confident that your model's performance on this data is representative of how it will perform on new, unseeen data.

### Feature Engineering

You might be skeptical of the high evaluation scores. There is a chance that there is some data leakage occurring. Data leakage is when you use data to train your model that should not be used during training, either because it appears in the test data or because it's not data that you'd expect to have when the model is actually deployed. Training a model with leaked data can give an unrealistic score that is not replicated in production.

In this case, it's likely that the company won't have satisfaction levels reported for all of its employees. It's also possible that the `average_monthly_hours` column is a source of some data leakage. If employees have already decided upon quitting, or have already been identified by management as people to be fired, they may be working fewer hours.

The first round of decision tree and random forest models included all variables as features. This next round will incorporate feature engineering to build improved models.

You could proceed by dropping `satisfaction_level` and creating a new feature that roughly captures whether an employee is overworked. You could call this new feature `overworked`. It will be a binary variable.
"""

# Drop `satisfaction_level` and save resulting dataframe in new variable
df2 = df_enc.drop('satisfaction_level', axis=1)

# Display first few rows of new dataframe
df2.head()

# Create `overworked` column. For now, it's identical to average monthly hours.
df2['overworked'] = df2['average_monthly_hours']

# Inspect max and min average monthly hours values
print('Max hours:', df2['overworked'].max())
print('Min hours:', df2['overworked'].min())

"""166.67 is approximately the average number of monthly hours for someone who works 50 weeks per year, 5 days per week, 8 hours per day.

You could define being overworked as working more than 175 hours per month on average.

To make the `overworked` column binary, you could reassign the column using a boolean mask.
- `df3['overworked'] > 175` creates a series of booleans, consisting of `True` for every value > 175 and `False` for every values ≤ 175
- `.astype(int)` converts all `True` to `1` and all `False` to `0`
"""

# Define `overworked` as working > 175 hrs/week
df2['overworked'] = (df2['overworked'] > 175).astype(int)

# Display first few rows of new column
df2['overworked'].head()

# Drop the `average_monthly_hours` column
df2 = df2.drop('average_monthly_hours', axis=1)

# Display first few rows of resulting dataframe
df2.head()

# Isolate the outcome variable
y = df2['left']

# Select the features
X = df2.drop('left', axis=1)

# Create test data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)

"""###### Decision Tree - Round 2"""

# Instantiate model
tree = DecisionTreeClassifier(random_state=0)

# Assign a dictionary of hyperparameters to search over
cv_params = {'max_depth':[4, 6, 8, None],
             'min_samples_leaf': [2, 5, 1],
             'min_samples_split': [2, 4, 6]
             }

# Assign a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}

# Instantiate GridSearch
tree2 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# tree2.fit(X_train, y_train)

# Check best params
tree2.best_params_

# Check best AUC score on CV
tree2.best_score_

"""This model performs very well, even without satisfaction levels and detailed hours worked data.

Next, check the other scores.
"""

# Get all CV scores
tree2_cv_results = make_results('decision tree2 cv', tree2, 'auc')
print(tree1_cv_results)
print(tree2_cv_results)

# Some of the other scores fell. That's to be expected given fewer features were taken into account in this round of the model. Still, the scores are very good.

"""###### Random Forest - Round 2"""

# Instantiate model
rf = RandomForestClassifier(random_state=0)

# Assign a dictionary of hyperparameters to search over
cv_params = {'max_depth': [3,5, None],
             'max_features': [1.0],
             'max_samples': [0.7, 1.0],
             'min_samples_leaf': [1,2,3],
             'min_samples_split': [2,3,4],
             'n_estimators': [300, 500],
             }

# Assign a dictionary of scoring metrics to capture
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}

# Instantiate GridSearch
rf2 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# rf2.fit(X_train, y_train) # --> Wall time: 7min 5s

# Write pickle
write_pickle(path, rf2, 'hr_rf2')

# Read in pickle
rf2 = read_pickle(path, 'hr_rf2')

# Check best params
rf2.best_params_

# Check best AUC score on CV
rf2.best_score_

# Get all CV scores
rf2_cv_results = make_results('random forest2 cv', rf2, 'auc')
print(tree2_cv_results)
print(rf2_cv_results)

"""Again, the scores dropped slightly, but the random forest performs better than the decision tree if using AUC as the deciding metric.

Score the champion model on the test set now.
"""

# Get predictions on test data
rf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)
rf2_test_scores

"""###### Get predictions on test data
rf2_test_scores = get_scores('random forest2 test', rf2, X_test, y_test)
rf2_test_scores
"""

# Generate array of values for confusion matrix
preds = rf2.best_estimator_.predict(X_test)
cm = confusion_matrix(y_test, preds, labels=rf2.classes_)

# Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                             display_labels=rf2.classes_)
disp.plot(values_format='');

"""##### Generate array of values for confusion matrix
preds = rf2.best_estimator_.predict(X_test)
cm = confusion_matrix(y_test, preds, labels=rf2.classes_)

##### Plot confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                             display_labels=rf2.classes_)
disp.plot(values_format='');
"""

# Plot the tree
plt.figure(figsize=(85,20))
plot_tree(tree2.best_estimator_, max_depth=6, fontsize=14, feature_names=X.columns,
          class_names={0:'stayed', 1:'left'}, filled=True);
plt.show()

#tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns=X.columns)
tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_,
                                 columns=['gini_importance'],
                                 index=X.columns
                                )
tree2_importances = tree2_importances.sort_values(by='gini_importance', ascending=False)

# Only extract the features with importances > 0
tree2_importances = tree2_importances[tree2_importances['gini_importance'] != 0]
tree2_importances

sns.barplot(data=tree2_importances, x="gini_importance", y=tree2_importances.index, orient='h')
plt.title("Decision Tree: Feature Importances for Employee Leaving", fontsize=12)
plt.ylabel("Feature")
plt.xlabel("Importance")
plt.show()

"""
The barplot above shows that in this decision tree model, `last_evaluation`, `number_project`, `tenure`, and `overworked` have the highest importance, in that order. These variables are most helpful in predicting the outcome variable, `left`."""

### Random Forest Freature importance

# Get feature importances
feat_impt = rf2.best_estimator_.feature_importances_

# Get indices of top 10 features
ind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]

# Get column labels of top 10 features
feat = X.columns[ind]

# Filter `feat_impt` to consist of top 10 feature importances
feat_impt = feat_impt[ind]

y_df = pd.DataFrame({"Feature":feat,"Importance":feat_impt})
y_sort_df = y_df.sort_values("Importance")
fig = plt.figure()
ax1 = fig.add_subplot(111)

y_sort_df.plot(kind='barh',ax=ax1,x="Feature",y="Importance")

ax1.set_title("Random Forest: Feature Importances for Employee Leaving", fontsize=12)
ax1.set_ylabel("Feature")
ax1.set_xlabel("Importance")

plt.show()

"""The plot above shows that in this random forest model, `last_evaluation`, `number_project`, `tenure`, and `overworked` have the highest importance, in that order. These variables are most helpful in predicting the outcome variable, `left`, and they are the same as the ones used by the decision tree model.

✏
## Recall evaluation metrics

- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example.
- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.
- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.
- **Accuracy** measures the proportion of data points that are correctly classified.
- **F1-score** is an aggregation of precision and recall.

### Results and Evaluation
- Interpret model
- Evaluate model performance using metrics
- Prepare results, visualizations, and actionable steps to share with stakeholders

### Summary of model results

**Logistic Regression**

The logistic regression model achieved precision of 80%, recall of 83%, f1-score of 80% (all weighted averages), and accuracy of 83%, on the test set.

**Tree-based Machine Learning**

After conducting feature engineering, the decision tree model achieved AUC of 93.8%, precision of 87.0%, recall of 90.4%, f1-score of 88.7%, and accuracy of 96.2%, on the test set. The random forest modestly outperformed the decision tree model.

### Conclusion, Recommendations, Next Steps

The models and the feature importances extracted from the models confirm that employees at the company are overworked.

To retain employees, the following recommendations could be presented to the stakeholders:

* Cap the number of projects that employees can work on.
* Consider promoting employees who have been with the company for atleast four years, or conduct further investigation about why four-year tenured employees are so dissatisfied.
* Either reward employees for working longer hours, or don't require them to do so.
* If employees aren't familiar with the company's overtime pay policies, inform them about this. If the expectations around workload and time off aren't explicit, make them clear.
* Hold company-wide and within-team discussions to understand and address the company work culture, across the board and in specific contexts.
* High evaluation scores should not be reserved for employees who work 200+ hours per month. Consider a proportionate scale for rewarding employees who contribute more/put in more effort.

**Next Steps**

It may be justified to still have some concern about data leakage. It could be prudent to consider how predictions change when `last_evaluation` is removed from the data. It's possible that evaluations aren't performed very frequently, in which case it would be useful to be able to predict employee retention without this feature. It's also possible that the evaluation score determines whether an employee leaves or stays, in which case it could be useful to pivot and try to predict performance score. The same could be said for satisfaction score.

For another project, you could try building a K-means model on this data and analyzing the clusters. This may yield valuable insight.
"""



